{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pointer Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "import math, operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "\n",
    "import os, time, copy\n",
    "import json\n",
    "import models.train_predict_utils as ut\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.batch_env_rl import BatchEnvRL\n",
    "\n",
    "from models.neural_net import Agent\n",
    "from models.run_episode import RunEpisode\n",
    "\n",
    "from models.features_utils import ScalerGlob, DynamicFeatures\n",
    "from generator.op.generator_utils import get_generated_seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "from torch import optim\n",
    "from torch import dot\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility\"\n",
    "random_seed = 25029\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainEpochs(run_episode, ds, model_opt, scheduler, args, run_vr ='0'):\n",
    "    \n",
    "    val_ranges = dict()\n",
    "    val_ranges[20] = [1, 250]\n",
    "    val_ranges[50] = [251, 500]\n",
    "    val_ranges[100] = [501, 750]\n",
    "    val_ranges[200] = [751, 1000]\n",
    "    \n",
    "    # Keep track of time elapsed and running averages\n",
    "    start = time.time()\n",
    "     \n",
    "    reward_total = 0 \n",
    "    tloss_total = 0\n",
    "    rwds_total = 0\n",
    "    pen_total = 0\n",
    "    train_hist = []\n",
    "    \n",
    "    gen_seeds = get_generated_seeds()\n",
    "    #to consider only up to seed 4000 for all n_nodes\n",
    "    gen_seeds[20] = np.array([s for s in gen_seeds.get(20) if s<=4000]) \n",
    "    \n",
    "    writer = SummaryWriter()\n",
    "    with tqdm(range(args.epochs), leave=False, desc='1th loop') as tepoch:\n",
    "        \n",
    "        step = 0\n",
    "        for epoch in tepoch:\n",
    "            avreward, tloss, avg_rwds, avg_pen = ut.train_model(run_episode, ds.data_scaler, model_opt, scheduler,\n",
    "                                                                         args, gen_seeds)\n",
    "            \n",
    "            reward_total += avreward\n",
    "            tloss_total += tloss\n",
    "            rwds_total += avg_rwds\n",
    "            pen_total += avg_pen\n",
    "            step +=1    \n",
    "            if (epoch+1) % args.nprint == 0:\n",
    "                taverage_loss = tloss_total / step\n",
    "                avreward_total = reward_total / step\n",
    "                avg_rwds_total = rwds_total / step\n",
    "                avg_pen_total = pen_total / step\n",
    "\n",
    "                print('epoch: {}, Av. loss: {:.3f}, Av. final reward: {:.3f}'.format(str(epoch+1), taverage_loss, avreward_total))\n",
    "                print('epoch: {}, Av. rwd: {:.3f}, Av. pen: {:.3f}'.format(str(epoch+1), avg_rwds_total, avg_pen_total))\n",
    "                tepoch.set_postfix(loss=taverage_loss, reward=avg_rwds_total, penalty=avg_pen_total, final=avreward_total)\n",
    "                time.sleep(0.1)\n",
    "                \n",
    "                tloss_total = 0\n",
    "                reward_total = 0\n",
    "                rwds_total = 0\n",
    "                pen_total = 0\n",
    "                step = 0\n",
    "  \n",
    "                step_dict = {}\n",
    "                step_dict['epoch'] = epoch+1\n",
    "                step_dict['tr_rwd'] = avg_rwds_total\n",
    "                step_dict['tr_pen'] = avg_pen_total\n",
    "                step_dict['tr_loss'] = taverage_loss\n",
    "                writer.add_scalar(f'tr_rwd', avg_rwds_total, epoch)\n",
    "                writer.add_scalar(f'tr_pen', avg_pen_total, epoch)\n",
    "                writer.add_scalar(f'tr_total', avg_rwds_total+avg_pen_total, epoch)\n",
    "                writer.add_scalar(f'tr_loss', taverage_loss, epoch)\n",
    "                \n",
    "                file_path = '{path}/train_hist_{agent_name}_noise_{noise}_{notebook_name}_r{run_vr}.csv'.format(path=args.save_hist_dir, \n",
    "                                               agent_name=args.agent_name,\n",
    "                                               noise=str(int(args.noise_on)),\n",
    "                                               notebook_name=args.nb_name,\n",
    "                                               run_vr=run_vr)\n",
    "                train_hist_df = pd.DataFrame(train_hist)\n",
    "                train_hist_df.to_csv(file_path, index=False)\n",
    "                \n",
    "                av_rws_total = 0\n",
    "                av_pens_total = 0\n",
    "                for n_nodes_val in [20, 50, 100, 200]:\n",
    "                    av_rwds, av_pens = ut.run_validation(run_episode, \n",
    "                                                         val_ranges.get(n_nodes_val)[0],\n",
    "                                                         val_ranges.get(n_nodes_val)[1], \n",
    "                                                         ds, args, which_set='test')\n",
    "                    print (f'validation {n_nodes_val} nodes - reward: {av_rwds:.2f}, penalty: {av_pens:.2f}, final: {(av_rwds+av_pens):.2f}')\n",
    "\n",
    "                    writer.add_scalar(f'rwds_val_{n_nodes_val}', av_rwds, epoch+1)\n",
    "                    writer.add_scalar(f'pens_val_{n_nodes_val}', av_pens, epoch+1)\n",
    "                    writer.add_scalar(f'total_val_{n_nodes_val}', av_rwds+av_pens, epoch+1)\n",
    "                    \n",
    "                    step_dict[f'val_rwd_{n_nodes_val}'] = av_rwds\n",
    "                    step_dict[f'val_pen_{n_nodes_val}'] = av_pens\n",
    "                    av_rws_total += av_rwds\n",
    "                    av_pens_total += av_pens\n",
    "                    \n",
    "                writer.add_scalar(f'rwds_val_all', av_rws_total/4, epoch+1)\n",
    "                writer.add_scalar(f'pens_val_all', av_pens_total/4, epoch+1)\n",
    "                writer.add_scalar(f'total_val_all', av_rws_total/4 + av_pens_total/4, epoch+1)\n",
    "                print(' ')\n",
    "                \n",
    "                train_hist.append(step_dict)\n",
    "\n",
    "            if (epoch+1) % args.nsave == 0:\n",
    "                file_path = '{path}/model_{agent_name}_noise_{noise}_{notebook_name}_epoch_{epoch}_r{run_vr}.pkl'\\\n",
    "                                            .format(path=args.save_weights_dir, \n",
    "                                                   agent_name=args.agent_name,\n",
    "                                                   noise=str(int(args.noise_on)),\n",
    "                                                   notebook_name=args.nb_name,\n",
    "                                                   epoch=epoch+1,\n",
    "                                                   run_vr=run_vr)\n",
    "                \n",
    "                \n",
    "                torch.save({\n",
    "                        'epoch': epoch+1,\n",
    "                        'model_state_dict': run_episode.state_dict(),\n",
    "                        'optimizer_state_dict': model_opt.state_dict()}, file_path)\n",
    "        writer.close()\n",
    "    return pd.DataFrame(train_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Config\n",
    "# ====================================================\n",
    "class args:\n",
    "    save_weights_dir = '../weights'\n",
    "    save_hist_dir = '../training_hist'\n",
    "    save_sub = '../submissions'\n",
    "    epochs = 15000\n",
    "    n_nodes_list = range(10, 210)\n",
    "    save_with_tr = True\n",
    "    nb_name = 'nb7p0p8p3rg'\n",
    "    agent_name = 'agent001'\n",
    "    nsave = 1000\n",
    "    ndfeatures = 34\n",
    "    lr = 1e-4\n",
    "    min_lr = 1e-5\n",
    "    batch_size = 32\n",
    "    weight_decay = 1e-5\n",
    "    max_grad_norm = 2\n",
    "    beta = 0.0 # for moving Av\n",
    "    gamma = 0.01 # for entropy \n",
    "    # Model parameters\n",
    "    rnn_hidden = 256  # dimension of decoder \n",
    "    encoder_dim = 256\n",
    "    pre_lnorm = False\n",
    "    has_glimpse = False\n",
    "    use_lookahead = True\n",
    "    dropout = 0.1\n",
    "    n_layers = 3\n",
    "    n_heads = 8\n",
    "    ff_dim = 512\n",
    "    use_cuda = True\n",
    "    device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "    use_checkpoint = True\n",
    "    nprint = 250\n",
    "    n_sims = 6\n",
    "    accumulation_steps = n_sims \n",
    "    from_file = True\n",
    "    noise_on = True\n",
    "    feature_list = ['x_coordinate',\n",
    "                    'y_coordinate',\n",
    "                    'tw_low',\n",
    "                    'tw_high',\n",
    "                    'prize',\n",
    "                    'tmax',\n",
    "                    'tw_delta',\n",
    "                    'prize_tw_delta_ratio',\n",
    "                    'tw_high_tmax_delta',\n",
    "                    'tw_low_tmax_delta',\n",
    "                    'prize_max_return_time_ratio']\n",
    "\n",
    "    nfeatures = len(feature_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunEpisode testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Agent(args.nfeatures, args.ndfeatures, args.rnn_hidden, args).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_episode = RunEpisode(model, args, DynamicFeatures, args.use_lookahead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ScalerGlob()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_opt = optim.AdamW(run_episode.parameters(), lr=args.lr)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(model_opt, T_max= args.epochs, \n",
    "                                                        eta_min=args.min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_hist = trainEpochs(run_episode, ds, model_opt, scheduler, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('{path}/model_{agent_name}_noise_{noise}_{notebook_name}_epoch_{epoch}_r0.pkl'.format(path=args.save_weights_dir, \n",
    "                                   agent_name=args.agent_name,\n",
    "                                   noise=str(int(args.noise_on)),\n",
    "                                   notebook_name=args.nb_name,\n",
    "                                   epoch=15000))\n",
    "run_episode.load_state_dict(checkpoint['model_state_dict'])\n",
    "model_opt.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "av_rwds, av_pens = ut.run_validation(run_episode, 1, 1000, ds, args, which_set='test')\n",
    "print(av_rwds+av_pens, av_rwds, av_pens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "av_rwds, av_pens = ut.create_submission(run_episode, ds, args, n_tours=100, with_as=False, which_set='test')\n",
    "print(av_rwds+av_pens, av_rwds, av_pens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "tsp-ai-competition",
   "language": "python",
   "name": "tsp-ai-competition"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
